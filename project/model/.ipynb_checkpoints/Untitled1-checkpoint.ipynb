{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#随机取样进行合并的做法\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import pdist\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_sample_data(df,path,n_clusters=10,percent=100):\n",
    "    df = df.loc[df.model == 2]\n",
    "    dffalse = df.loc[df.label == 0].reset_index(drop=True)\n",
    "    dftrue = df.loc[df.label == 1].reset_index(drop=True)\n",
    "    drop = ['smart_10raw','smart_12raw','smart_184raw','smart_187raw','smart_188raw','smart_189raw','smart_190raw','smart_191raw','smart_192raw','smart_193raw','smart_194raw','smart_195raw','smart_197raw','smart_198raw','smart_199raw','smart_1raw','smart_240raw','smart_241raw','smart_242raw','smart_3raw','smart_4raw','smart_5raw','smart_7raw','smart_9raw']\n",
    "    drop_list =[]\n",
    "    for i in tqdm([col for col in df.columns if col not in [drop,'label','serial_number','dt','model','fault_time','diff_day','manufacturer']]):\n",
    "        drop_list.append(i)\n",
    "    drop_list = list(set(drop_list)-set(drop))\n",
    "    df2= dffalse[drop_list]\n",
    "    df2 = df2.fillna(method='ffill')\n",
    "    df2 = df2.fillna(method='bfill')\n",
    "    cluster = KMeans(n_clusters, random_state = 0).fit(df2)\n",
    "    dffalse['clul'] = cluster.labels_\n",
    "    dffalse['clui'] = cluster.inertia_  \n",
    "    cen = cluster.cluster_centers_\n",
    "    cen1 = pd.DataFrame(cen,columns=df2.columns)\n",
    "    dftemp = dffalse['clul']\n",
    "    dfsize = (int)(df2.index.size)\n",
    "    d1 = np.array([])\n",
    "    for i in tqdm(range(dfsize)):\n",
    "        d = pdist(np.vstack([df2.iloc[i],cen1.iloc[(dftemp.iloc[i])]]))\n",
    "        d1 = np.append(d1,d)\n",
    "    dover = pd.Series(d1)\n",
    "    dffalse.clui=dover\n",
    "    dffalse = dffalse.sort_values(['clul','clui']).reset_index(drop=True)\n",
    "    dffalsetrue = pd.concat([dffalse, dftrue]).reset_index(drop=True)\n",
    "    joblib.dump(dffalsetrue, path)\n",
    "    number = percent * (len(dftrue)) \n",
    "    size = (int)(len(dffalse)/number)\n",
    "    df3 = dffalse.iloc[::size].reset_index(drop=True)\n",
    "    df4 = pd.concat([df3, dftrue]).reset_index(drop=True)\n",
    "    drop_over =[]\n",
    "    for i in tqdm([col for col in df4.columns if col not in ['clul','clui']]):\n",
    "        drop_over.append(i)\n",
    "    df5 = df4[drop_over].reset_index(drop=True)\n",
    "    return df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 48/48 [00:00<00:00, 24044.74it/s]\n",
      " 14%|██████████▍                                                             | 206100/1425376 [04:54<55:38, 365.22it/s]"
     ]
    }
   ],
   "source": [
    "train_2018_2 = joblib.load('../user_data/train_2018_2.jl.z')\n",
    "path = '../user_data/train_2018_21.jl.z'\n",
    "train_2018_2=lower_sample_data(train_2018_2,path)\n",
    "train_2018_2.to_csv(\"../user_data/train_2018_2.csv\",index=False)\n",
    "del train_2018_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #生成新的文件，测试集\n",
    "# train_2018_6 = joblib.load('../user_data/train_2018_6.jl.z')\n",
    "# train_x = train_2018_6\n",
    "# train_x.to_csv(\"../user_data/train_x.csv\",index=False)\n",
    "\n",
    "#初步调参\n",
    "train_2018_2 = pd.read_csv('../user_data/train_2018_2.csv')\n",
    "train_2018_3 = pd.read_csv('../user_data/train_2018_3.csv')\n",
    "val_x = train_2018_2\n",
    "val_x = val_x.append(train_2018_3).reset_index(drop=True)\n",
    "val_x.to_csv(\"../user_data/val_x.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2017_7 = pd.read_csv('../user_data/train_2017_7.csv')\n",
    "train_2017_8 = pd.read_csv('../user_data/train_2017_8.csv')\n",
    "train_2017_9 = pd.read_csv('../user_data/train_2017_9.csv')\n",
    "train_2017_10 = pd.read_csv('../user_data/train_2017_10.csv')\n",
    "train_2017_11 = pd.read_csv('../user_data/train_2017_11.csv')\n",
    "train_2017_12 = pd.read_csv('../user_data/train_2017_12.csv')\n",
    "train_2018_1 = pd.read_csv('../user_data/train_2018_1.csv')\n",
    "train_2018_2 = pd.read_csv('../user_data/train_2018_2.csv')\n",
    "train_2018_3 = pd.read_csv('../user_data/train_2018_3.csv')\n",
    "train_2018_4 = pd.read_csv('../user_data/train_2018_4.csv')\n",
    "train_2018_5 = pd.read_csv('../user_data/train_2018_5.csv')\n",
    "train_2018_6 = pd.read_csv('../user_data/train_2018_6.csv')\n",
    "train_2018_7 = joblib.load('../user_data/train_2018_7.jl.z')\n",
    "train_x = train_2017_7\n",
    "train_x = train_x.append(train_2017_8).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2017_9).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2017_10).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2017_11).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2017_12).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2018_1).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2018_2).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2018_3).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2018_4).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2018_5).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2018_6).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2018_7).reset_index(drop=True)\n",
    "train_x.to_csv(\"../user_data/train_df.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 512/512 [00:01<00:00, 501.38it/s]\n"
     ]
    }
   ],
   "source": [
    "#训练数据\n",
    "test = pd.read_csv('../data/test_b.csv')\n",
    "tag = pd.read_csv('../data/tag.csv')\n",
    "test['dt'] = test['dt'].apply(lambda x:''.join(str(x)[0:4] +'-'+ str(x)[4:6]  +'-'+ str(x)[6:]))\n",
    "test['dt'] = pd.to_datetime(test['dt'])\n",
    "tag['fault_time'] = pd.to_datetime(tag['fault_time'])\n",
    "\n",
    "test = test.sort_values(['serial_number','dt'])\n",
    "test = test.drop_duplicates().reset_index(drop=True)\n",
    "sub = test[['manufacturer','model','serial_number','dt']]\n",
    "\n",
    "drop_list =[]\n",
    "for i in tqdm([col for col in test.columns if col not in ['manufacturer','model']]):\n",
    "    if (test[i].nunique() == 1)&(test[i].isnull().sum() == 0):\n",
    "        drop_list.append(i)\n",
    "\n",
    "df= pd.DataFrame()\n",
    "df['fea'] = test.isnull().sum().index\n",
    "df['isnull_sum'] = test.isnull().sum().values\n",
    "fea_list = list(set(df.loc[df.isnull_sum != test.shape[0]]['fea']) - set(drop_list))\n",
    "test = test[fea_list]\n",
    "\n",
    "serial = pd.read_csv('../user_data/serial.csv')\n",
    "serial.columns = ['serial_number','dt_first','model']\n",
    "serial.dt_first = pd.to_datetime(serial.dt_first)\n",
    "\n",
    "tag['tag'] = tag['tag'].astype(str)\n",
    "tag = tag.groupby(['serial_number','fault_time','model'])['tag'].apply(lambda x :'|'.join(x)).reset_index()\n",
    "tag.columns = ['serial_number','fault_time_1','model','tag']\n",
    "map_dict = dict(zip(tag['tag'].unique(), range(tag['tag'].nunique())))\n",
    "tag['tag'] = tag['tag'].map(map_dict).fillna(-1).astype('int32')\n",
    "\n",
    "###用到的特征\n",
    "feature_name = [i for i in test.columns if i not in ['dt','manufacturer']] + ['days','days_1','days_2','tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = joblib.load('../user_data/train_2018_6.jl.z')\n",
    "train_x = train_x.merge(serial,how = 'left',on = ['serial_number','model'])\n",
    "###硬盘的使用时常\n",
    "train_x['dt'] = pd.to_datetime(train_x['dt'],format = '%Y-%m-%d %H:%M:%S')\n",
    "train_x['days'] = (train_x['dt'] - train_x['dt_first']).dt.days\n",
    "\n",
    "train_x = train_x.merge(tag,how = 'left',on = ['serial_number','model'])\n",
    "###当前时间与另一个model故障的时间差，\n",
    "train_x['days_1'] = (train_x['dt'] - train_x['fault_time_1']).dt.days\n",
    "train_x.loc[train_x.days_1 <= 0,'tag'] = None\n",
    "train_x.loc[train_x.days_1 <= 0,'days_1'] = None\n",
    "\n",
    "###同一硬盘第一次使用到开始故障的时间\n",
    "train_x['days_2'] = (train_x['fault_time_1'] - train_x['dt_first']).dt.days\n",
    "train_x.loc[train_x.fault_time_1 >= train_x.dt,'days_2'] = None\n",
    "\n",
    "train_x['serial_number'] = train_x['serial_number'].apply(lambda x:int(x.split('_')[1]))\n",
    "train_y = train_x.label.values\n",
    "train_x = train_x[feature_name]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_x = pd.read_csv('../user_data/val_x.csv')\n",
    "val_x = val_x.merge(serial,how = 'left',on = ['serial_number','model'])\n",
    "val_x['dt'] = pd.to_datetime(val_x['dt'],format = '%Y-%m-%d %H:%M:%S')\n",
    "val_x['dt_first'] = pd.to_datetime(val_x['dt_first'],format = '%Y-%m-%d %H:%M:%S')\n",
    "val_x['days'] = (val_x['dt'] - val_x['dt_first']).dt.days\n",
    "\n",
    "\n",
    "val_x = val_x.merge(tag,how = 'left',on = ['serial_number','model'])\n",
    "val_x['days_1'] = (val_x['dt'] - val_x['fault_time_1']).dt.days\n",
    "val_x.loc[val_x.days_1 <= 0,'tag'] = None\n",
    "val_x.loc[val_x.days_1 <= 0,'days_1'] = None\n",
    "val_x['days_2'] = (val_x['fault_time_1'] - val_x['dt_first']).dt.days\n",
    "val_x.loc[val_x.fault_time_1 >= val_x.dt,'days_2'] = None\n",
    "\n",
    "val_x['serial_number'] = val_x['serial_number'].apply(lambda x:int(x.split('_')[1]))\n",
    "val_y = val_x.label.values\n",
    "val_x = val_x[feature_name]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************** training **************\n",
      "(4647434, 54) (605697, 54)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's auc: 0.71778\n",
      "Early stopping, best iteration is:\n",
      "[61]\tvalid_0's auc: 0.721037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.8,\n",
       "               importance_type='split', is_unbalenced='True',\n",
       "               learning_rate=0.001, max_depth=-1, metric=None,\n",
       "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=10000, n_jobs=-1, num_leaves=127, objective=None,\n",
       "               random_state=2019, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "               subsample=0.8, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "clf = LGBMClassifier(\n",
    "    learning_rate=0.001,\n",
    "    n_estimators=10000,\n",
    "    num_leaves=127,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=2019,\n",
    "    is_unbalenced = 'True',\n",
    "    metric=None\n",
    ")\n",
    "print('************** training **************')\n",
    "print(train_x.shape,val_x.shape)\n",
    "clf.fit(\n",
    "    train_x, train_y,\n",
    "    eval_set=[(val_x, val_y)],\n",
    "    eval_metric='auc',\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../user_data/train_df.csv')\n",
    "train_df = train_df.merge(serial,how = 'left',on = ['serial_number','model'])\n",
    "train_df['dt'] = pd.to_datetime(train_df['dt'],format = '%Y-%m-%d %H:%M:%S')\n",
    "train_df['dt_first'] = pd.to_datetime(train_df['dt_first'],format = '%Y-%m-%d %H:%M:%S')\n",
    "train_df['days'] = (train_df['dt'] - train_df['dt_first']).dt.days\n",
    "\n",
    "train_df = train_df.merge(tag,how = 'left',on = ['serial_number','model'])\n",
    "train_df['days_1'] = (train_df['dt'] - train_df['fault_time_1']).dt.days\n",
    "train_df.loc[train_df.days_1 <= 0,'tag'] = None\n",
    "train_df.loc[train_df.days_1 <= 0,'days_1'] = None\n",
    "train_df['days_2'] = (train_df['fault_time_1'] - train_df['dt_first']).dt.days\n",
    "train_df.loc[train_df.fault_time_1 >= train_df.dt,'days_2'] = None\n",
    "\n",
    "\n",
    "train_df['serial_number'] = train_df['serial_number'].apply(lambda x:int(x.split('_')[1]))\n",
    "labels = train_df.label.values\n",
    "train_df = train_df[feature_name]\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "test = test.merge(serial,how = 'left',on =  ['serial_number','model'])\n",
    "test['days'] = (test['dt'] - test['dt_first']).dt.days\n",
    "\n",
    "test = test.merge(tag,how = 'left',on = ['serial_number','model'])\n",
    "test['days_1'] = (test['dt'] - test['fault_time_1']).dt.days\n",
    "test.loc[test.days_1 <= 0,'tag'] = None\n",
    "test.loc[test.days_1 <= 0,'days_1'] = None\n",
    "test['days_2'] = (test['fault_time_1'] - test['dt_first']).dt.days\n",
    "test.loc[test.fault_time_1 >= test.dt,'days_2'] = None\n",
    "\n",
    "\n",
    "test['serial_number'] = test['serial_number'].apply(lambda x:int(x.split('_')[1]))\n",
    "test_x = test[feature_name]\n",
    "\n",
    "\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************** training **************\n",
      "(2523818, 54) (170442, 54)\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's auc: 0.788818\n",
      "[200]\ttraining's auc: 0.814282\n",
      "[300]\ttraining's auc: 0.850312\n",
      "[400]\ttraining's auc: 0.876589\n",
      "[500]\ttraining's auc: 0.887388\n",
      "[600]\ttraining's auc: 0.903219\n",
      "[700]\ttraining's auc: 0.914643\n",
      "[800]\ttraining's auc: 0.922692\n",
      "[900]\ttraining's auc: 0.931851\n",
      "[1000]\ttraining's auc: 0.939957\n",
      "[1100]\ttraining's auc: 0.944313\n",
      "[1200]\ttraining's auc: 0.948517\n",
      "[1300]\ttraining's auc: 0.951965\n",
      "[1400]\ttraining's auc: 0.956012\n",
      "[1500]\ttraining's auc: 0.960014\n",
      "[1600]\ttraining's auc: 0.966947\n",
      "[1700]\ttraining's auc: 0.973271\n",
      "[1800]\ttraining's auc: 0.978419\n",
      "[1900]\ttraining's auc: 0.981084\n",
      "[2000]\ttraining's auc: 0.983743\n",
      "[2100]\ttraining's auc: 0.986465\n",
      "[2200]\ttraining's auc: 0.988087\n",
      "[2300]\ttraining's auc: 0.989443\n",
      "[2400]\ttraining's auc: 0.990059\n",
      "[2500]\ttraining's auc: 0.991832\n",
      "[2600]\ttraining's auc: 0.992913\n",
      "[2700]\ttraining's auc: 0.993514\n",
      "[2800]\ttraining's auc: 0.993894\n",
      "[2900]\ttraining's auc: 0.994147\n",
      "[3000]\ttraining's auc: 0.994427\n",
      "[3100]\ttraining's auc: 0.994725\n",
      "[3200]\ttraining's auc: 0.994904\n",
      "[3300]\ttraining's auc: 0.995129\n",
      "[3400]\ttraining's auc: 0.995268\n",
      "[3500]\ttraining's auc: 0.995393\n",
      "[3600]\ttraining's auc: 0.995526\n",
      "[3700]\ttraining's auc: 0.995708\n",
      "[3800]\ttraining's auc: 0.995923\n",
      "[3900]\ttraining's auc: 0.996195\n",
      "[4000]\ttraining's auc: 0.996569\n",
      "[4100]\ttraining's auc: 0.996923\n",
      "[4200]\ttraining's auc: 0.997091\n",
      "[4300]\ttraining's auc: 0.997274\n",
      "[4400]\ttraining's auc: 0.997385\n",
      "[4500]\ttraining's auc: 0.997528\n",
      "[4600]\ttraining's auc: 0.997644\n",
      "[4700]\ttraining's auc: 0.997711\n",
      "[4800]\ttraining's auc: 0.997862\n",
      "[4900]\ttraining's auc: 0.997936\n",
      "[5000]\ttraining's auc: 0.997996\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's auc: 0.997996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.8,\n",
       "               importance_type='split', learning_rate=0.001, max_depth=-1,\n",
       "               metric=None, min_child_samples=20, min_child_weight=0.001,\n",
       "               min_split_gain=0.0, n_estimators=5000, n_jobs=-1, num_leaves=127,\n",
       "               objective=None, random_state=2019, reg_alpha=0.0, reg_lambda=0.0,\n",
       "               silent=True, subsample=0.8, subsample_for_bin=200000,\n",
       "               subsample_freq=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LGBMClassifier(\n",
    "    learning_rate=0.001,\n",
    "    n_estimators=5000,\n",
    "    num_leaves=127,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=2019,\n",
    "#     is_unbalenced = 'True',\n",
    "    metric=None\n",
    ")\n",
    "print('************** training **************')\n",
    "print(train_df.shape,test_x.shape)\n",
    "clf.fit(\n",
    "    train_df, labels,\n",
    "    eval_set=[(train_df, labels)],\n",
    "    eval_metric='auc',\n",
    "    early_stopping_rounds=10,\n",
    "    verbose=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 60)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joblib.dump(clf, '../user_data/clf317.pkl')\n",
    "# print('xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')\n",
    "\n",
    "test['p'] = clf.predict_proba(test_x)[:,1]\n",
    "test['label'] = test['p'].rank()\n",
    "test['label']= (test['label']>=test.shape[0] * 0.990).astype(int)\n",
    "submit = test.loc[test.label == 1]\n",
    "submit = submit.sort_values('p',ascending=False)\n",
    "submit = submit.drop_duplicates(['serial_number','model'])\n",
    "submit['serial_number'] = submit['serial_number'].astype(str)\n",
    "submit['serial_number'] = ['disk_'+ x for x in submit['serial_number']]\n",
    "submit = submit[0:100]\n",
    "submit[['manufacturer','model','serial_number','dt']].to_csv(\"../prediction_result/predictions.csv\",index=False,header = None)\n",
    "submit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
