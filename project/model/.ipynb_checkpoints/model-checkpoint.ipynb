{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#随机取样进行合并的做法\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import pdist\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "def lower_sample_data(df,path,n_clusters=10,percent=100):\n",
    "    df = df.loc[df.model == 2]\n",
    "    dffalse = df.loc[df.label == 0].reset_index(drop=True)\n",
    "    dftrue = df.loc[df.label == 1].reset_index(drop=True)\n",
    "    drop = ['smart_10raw','smart_12raw','smart_184raw','smart_187raw','smart_188raw','smart_189raw','smart_190raw','smart_191raw','smart_192raw','smart_193raw','smart_194raw','smart_195raw','smart_197raw','smart_198raw','smart_199raw','smart_1raw','smart_240raw','smart_241raw','smart_242raw','smart_3raw','smart_4raw','smart_5raw','smart_7raw','smart_9raw']\n",
    "    drop_list =[]\n",
    "    for i in tqdm([col for col in df.columns if col not in [drop,'label','serial_number','dt','model','fault_time','diff_day','manufacturer']]):\n",
    "        drop_list.append(i)\n",
    "    drop_list = list(set(drop_list)-set(drop))\n",
    "    df2= dffalse[drop_list]\n",
    "    df2 = df2.fillna(method='ffill')\n",
    "    df2 = df2.fillna(method='bfill')\n",
    "    cluster = KMeans(n_clusters, random_state = 0).fit(df2)\n",
    "    dffalse['clul'] = cluster.labels_\n",
    "    dffalse['clui'] = cluster.inertia_  \n",
    "    cen = cluster.cluster_centers_\n",
    "    cen1 = pd.DataFrame(cen,columns=df2.columns)\n",
    "    dftemp = dffalse['clul']\n",
    "    dfsize = (int)(df2.index.size)\n",
    "    for i in tqdm(range(dfsize)):\n",
    "        X=np.vstack([df2.iloc[i],cen1.iloc[(dftemp.iloc[i])]])\n",
    "        dffalse.iloc[i,56]=pdist(X)\n",
    "    dffalse = dffalse.sort_values(['clul','clui']).reset_index(drop=True)\n",
    "    dffalsetrue = pd.concat([dffalse, dftrue]).reset_index(drop=True)\n",
    "    joblib.dump(dffalsetrue, path)\n",
    "    number = percent * (len(dftrue)) \n",
    "    size = (int)(len(dffalse)/number)\n",
    "    df3 = dffalse.iloc[::size].reset_index(drop=True)\n",
    "    df4 = pd.concat([df3, dftrue]).reset_index(drop=True)\n",
    "    drop_over =[]\n",
    "    for i in tqdm([col for col in df4.columns if col not in ['clul','clui']]):\n",
    "        drop_over.append(i)\n",
    "    df5 = df4[drop_over].reset_index(drop=True)\n",
    "    return df5\n",
    "\n",
    "train_2018_6 = joblib.load('../user_data/train_2018_61.jl.z')\n",
    "train_2018_6=lower_sample_data(train_2018_6)\n",
    "train_2018_6.to_csv(\"../user_data/train_2018_6.csv\",index=False)\n",
    "del train_2018_6\n",
    "\n",
    "train_2018_5 = joblib.load('../user_data/train_2018_51.jl.z')\n",
    "train_2018_5=lower_sample_data(train_2018_5)\n",
    "train_2018_5.to_csv(\"../user_data/train_2018_5.csv\",index=False)\n",
    "del train_2018_5\n",
    "\n",
    "train_2018_4 = joblib.load('../user_data/train_2018_41.jl.z')\n",
    "train_2018_4=lower_sample_data(train_2018_4)\n",
    "train_2018_4.to_csv(\"../user_data/train_2018_4.csv\",index=False)\n",
    "del train_2018_4\n",
    "\n",
    "train_2018_3 = joblib.load('../user_data/train_2018_31.jl.z')\n",
    "train_2018_3=lower_sample_data(train_2018_3)\n",
    "train_2018_3.to_csv(\"../user_data/train_2018_3.csv\",index=False)\n",
    "del train_2018_3\n",
    "\n",
    "train_2018_2 = joblib.load('../user_data/train_2018_21.jl.z')\n",
    "train_2018_2=lower_sample_data(train_2018_2)\n",
    "train_2018_2.to_csv(\"../user_data/train_2018_2.csv\",index=False)\n",
    "del train_2018_2\n",
    "\n",
    "train_2018_1 = joblib.load('../user_data/train_2018_11.jl.z')\n",
    "train_2018_1=lower_sample_data(train_2018_1)\n",
    "train_2018_1.to_csv(\"../user_data/train_2018_1.csv\",index=False)\n",
    "del train_2018_1\n",
    "\n",
    "train_2017_7 =  joblib.load('../user_data/train_2017_71.jl.z')\n",
    "train_2017_7=lower_sample_data(train_2017_7)\n",
    "train_2017_7.to_csv(\"../user_data/train_2017_7.csv\",index=False)\n",
    "del train_2017_7\n",
    "\n",
    "train_2017_8 =  joblib.load('../user_data/train_2017_81.jl.z')\n",
    "train_2017_8=lower_sample_data(train_2017_8)\n",
    "train_2017_8.to_csv(\"../user_data/train_2017_8.csv\",index=False)\n",
    "del train_2017_8\n",
    "\n",
    "train_2017_9 =  joblib.load('../user_data/train_2017_91.jl.z')\n",
    "train_2017_9=lower_sample_data(train_2017_9)\n",
    "train_2017_9.to_csv(\"../user_data/train_2017_9.csv\",index=False)\n",
    "del train_2017_9\n",
    "\n",
    "train_2017_10 = joblib.load('../user_data/train_2017_101.jl.z')\n",
    "train_2017_10=lower_sample_data(train_2017_10)\n",
    "train_2017_10.to_csv(\"../user_data/train_2017_10.csv\",index=False)\n",
    "del train_2017_10\n",
    "\n",
    "train_2017_11 = joblib.load('../user_data/train_2017_111.jl.z')\n",
    "train_2017_11=lower_sample_data(train_2017_11)\n",
    "train_2017_11.to_csv(\"../user_data/train_2017_11.csv\",index=False)\n",
    "del train_2017_11\n",
    "\n",
    "train_2017_12 = joblib.load('../user_data/train_2017_121.jl.z')\n",
    "train_2017_12=lower_sample_data(train_2017_12)\n",
    "train_2017_12.to_csv(\"../user_data/train_2017_12.csv\",index=False)\n",
    "del train_2017_12\n",
    "\n",
    "#生成新的文件\n",
    "train_2018_4 = pd.read_csv('../user_data/train_2018_4.csv')\n",
    "train_2018_5 = pd.read_csv('../user_data/train_2018_5.csv')\n",
    "train_2018_6 = pd.read_csv('../user_data/train_2018_6.csv')\n",
    "train_x = train_2018_4\n",
    "train_x = train_x.append(train_2018_5).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2018_6).reset_index(drop=True)\n",
    "train_x.to_csv(\"../user_data/train_x.csv\",index=False)\n",
    "\n",
    "train_2018_2 = pd.read_csv('../user_data/train_2018_2.csv')\n",
    "train_2018_3 = pd.read_csv('../user_data/train_2018_3.csv')\n",
    "val_x = train_2018_2\n",
    "val_x = train_x.append(train_2018_3).reset_index(drop=True)\n",
    "val_x.to_csv(\"../user_data/val_x.csv\",index=False)\n",
    "\n",
    "train_2017_7 = pd.read_csv('../user_data/train_2017_7.csv')\n",
    "train_2017_8 = pd.read_csv('../user_data/train_2017_8.csv')\n",
    "train_2017_9 = pd.read_csv('../user_data/train_2017_9.csv')\n",
    "train_2017_10 = pd.read_csv('../user_data/train_2017_10.csv')\n",
    "train_2017_11 = pd.read_csv('../user_data/train_2017_11.csv')\n",
    "train_2017_12 = pd.read_csv('../user_data/train_2017_12.csv')\n",
    "train_2018_1 = pd.read_csv('../user_data/train_2018_1.csv')\n",
    "train_2018_2 = pd.read_csv('../user_data/train_2018_2.csv')\n",
    "train_2018_3 = pd.read_csv('../user_data/train_2018_3.csv')\n",
    "train_2018_4 = pd.read_csv('../user_data/train_2018_4.csv')\n",
    "train_2018_5 = pd.read_csv('../user_data/train_2018_5.csv')\n",
    "train_2018_6 = pd.read_csv('../user_data/train_2018_6.csv')\n",
    "train_2018_7 = joblib.load('../user_data/train_2018_7.jl.z')\n",
    "train_x = train_2017_7\n",
    "train_x = train_x.append(train_2017_8).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2017_9).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2017_10).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2017_11).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2017_12).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2018_1).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2018_2).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2018_3).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2018_4).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2018_5).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2018_6).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2018_7).reset_index(drop=True)\n",
    "train_x.to_csv(\"../user_data/train_df.csv\",index=False)\n",
    "\n",
    "#训练数据\n",
    "test = pd.read_csv('../data/test_b.csv')\n",
    "tag = pd.read_csv('../data/tag.csv')\n",
    "tag = tag.loc[tag.model==2]\n",
    "test['dt'] = test['dt'].apply(lambda x:''.join(str(x)[0:4] +'-'+ str(x)[4:6]  +'-'+ str(x)[6:]))\n",
    "test['dt'] = pd.to_datetime(test['dt'])\n",
    "tag['fault_time'] = pd.to_datetime(tag['fault_time'])\n",
    "\n",
    "test = test.sort_values(['serial_number','dt'])\n",
    "test = test.drop_duplicates().reset_index(drop=True)\n",
    "sub = test[['manufacturer','model','serial_number','dt']]\n",
    "\n",
    "drop_list =[]\n",
    "for i in tqdm([col for col in test.columns if col not in ['manufacturer','model']]):\n",
    "    if (test[i].nunique() == 1)&(test[i].isnull().sum() == 0):\n",
    "        drop_list.append(i)\n",
    "\n",
    "df= pd.DataFrame()\n",
    "df['fea'] = test.isnull().sum().index\n",
    "df['isnull_sum'] = test.isnull().sum().values\n",
    "fea_list = list(set(df.loc[df.isnull_sum != test.shape[0]]['fea']) - set(drop_list))\n",
    "test = test[fea_list]\n",
    "\n",
    "serial = pd.read_csv('../user_data/serial.csv')\n",
    "serial.columns = ['serial_number','dt_first','model']\n",
    "serial.dt_first = pd.to_datetime(serial.dt_first)\n",
    "\n",
    "tag['tag'] = tag['tag'].astype(str)\n",
    "tag = tag.groupby(['serial_number','fault_time','model'])['tag'].apply(lambda x :'|'.join(x)).reset_index()\n",
    "tag.columns = ['serial_number','fault_time_1','model','tag']\n",
    "map_dict = dict(zip(tag['tag'].unique(), range(tag['tag'].nunique())))\n",
    "tag['tag'] = tag['tag'].map(map_dict).fillna(-1).astype('int32')\n",
    "\n",
    "###用到的特征\n",
    "feature_name = [i for i in test.columns if i not in ['dt','manufacturer']] + ['days','days_1','days_2','tag']\n",
    "\n",
    "train_x = pd.read_csv('../user_data/train_x.csv')\n",
    "train_x = train_x.merge(serial,how = 'left',on = ['serial_number','model'])\n",
    "###硬盘的使用时常\n",
    "train_x['dt'] = pd.to_datetime(train_x['dt'],format = '%Y-%m-%d %H:%M:%S')\n",
    "train_x['days'] = (train_x['dt'] - train_x['dt_first']).dt.days\n",
    "\n",
    "train_x = train_x.merge(tag,how = 'left',on = ['serial_number','model'])\n",
    "###当前时间与另一个model故障的时间差，\n",
    "train_x['days_1'] = (train_x['dt'] - train_x['fault_time_1']).dt.days\n",
    "train_x.loc[train_x.days_1 <= 0,'tag'] = None\n",
    "train_x.loc[train_x.days_1 <= 0,'days_1'] = None\n",
    "\n",
    "###同一硬盘第一次使用到开始故障的时间\n",
    "train_x['days_2'] = (train_x['fault_time_1'] - train_x['dt_first']).dt.days\n",
    "train_x.loc[train_x.fault_time_1 >= train_x.dt,'days_2'] = None\n",
    "\n",
    "train_x['serial_number'] = train_x['serial_number'].apply(lambda x:int(x.split('_')[1]))\n",
    "train_y = train_x.label.values\n",
    "train_x = train_x[feature_name]\n",
    "gc.collect()\n",
    "\n",
    "val_x = pd.read_csv('../user_data/val_x.csv')\n",
    "val_x = val_x.merge(serial,how = 'left',on = ['serial_number','model'])\n",
    "val_x['dt'] = pd.to_datetime(val_x['dt'],format = '%Y-%m-%d %H:%M:%S')\n",
    "val_x['dt_first'] = pd.to_datetime(val_x['dt_first'],format = '%Y-%m-%d %H:%M:%S')\n",
    "val_x['days'] = (val_x['dt'] - val_x['dt_first']).dt.days\n",
    "\n",
    "\n",
    "val_x = val_x.merge(tag,how = 'left',on = ['serial_number','model'])\n",
    "val_x['days_1'] = (val_x['dt'] - val_x['fault_time_1']).dt.days\n",
    "val_x.loc[val_x.days_1 <= 0,'tag'] = None\n",
    "val_x.loc[val_x.days_1 <= 0,'days_1'] = None\n",
    "val_x['days_2'] = (val_x['fault_time_1'] - val_x['dt_first']).dt.days\n",
    "val_x.loc[val_x.fault_time_1 >= val_x.dt,'days_2'] = None\n",
    "\n",
    "val_x['serial_number'] = val_x['serial_number'].apply(lambda x:int(x.split('_')[1]))\n",
    "val_y = val_x.label.values\n",
    "val_x = val_x[feature_name]\n",
    "gc.collect()\n",
    "\n",
    "gc.collect()\n",
    "clf = LGBMClassifier(\n",
    "    learning_rate=0.001,\n",
    "    n_estimators=10000,\n",
    "    num_leaves=125,\n",
    "    max_depth=7,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=2019,\n",
    "    bagging_fraction= 0.9,\n",
    "    bagging_freq= 8,\n",
    "    lambda_l1 = 0.5,\n",
    "    lambda_l2 = 0,\n",
    "    cat_smooth = 10, \n",
    "    is_unbalenced = 'True',\n",
    "    metric=None\n",
    ")    \n",
    "    \n",
    "#     learning_rate=0.001,\n",
    "#     n_estimators=10000,\n",
    "#     num_leaves=127,\n",
    "#     subsample=0.8,\n",
    "#     colsample_bytree=0.8,\n",
    "#     random_state=2019,\n",
    "#     is_unbalenced = 'True',\n",
    "#     metric=None\n",
    "\n",
    "print('************** training **************')\n",
    "print(train_x.shape,val_x.shape)\n",
    "clf.fit(\n",
    "    train_x, train_y,\n",
    "    eval_set=[(val_x, val_y)],\n",
    "    eval_metric='auc',\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=100\n",
    ")\n",
    "train_df = pd.read_csv('../user_data/train_df.csv')\n",
    "train_df = train_df.merge(serial,how = 'left',on = ['serial_number','model'])\n",
    "train_df['dt'] = pd.to_datetime(train_df['dt'],format = '%Y-%m-%d %H:%M:%S')\n",
    "train_df['dt_first'] = pd.to_datetime(train_df['dt_first'],format = '%Y-%m-%d %H:%M:%S')\n",
    "train_df['days'] = (train_df['dt'] - train_df['dt_first']).dt.days\n",
    "\n",
    "train_df = train_df.merge(tag,how = 'left',on = ['serial_number','model'])\n",
    "train_df['days_1'] = (train_df['dt'] - train_df['fault_time_1']).dt.days\n",
    "train_df.loc[train_df.days_1 <= 0,'tag'] = None\n",
    "train_df.loc[train_df.days_1 <= 0,'days_1'] = None\n",
    "train_df['days_2'] = (train_df['fault_time_1'] - train_df['dt_first']).dt.days\n",
    "train_df.loc[train_df.fault_time_1 >= train_df.dt,'days_2'] = None\n",
    "\n",
    "\n",
    "train_df['serial_number'] = train_df['serial_number'].apply(lambda x:int(x.split('_')[1]))\n",
    "labels = train_df.label.values\n",
    "train_df = train_df[feature_name]\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "test = test.merge(serial,how = 'left',on =  ['serial_number','model'])\n",
    "test['days'] = (test['dt'] - test['dt_first']).dt.days\n",
    "\n",
    "test = test.merge(tag,how = 'left',on = ['serial_number','model'])\n",
    "test['days_1'] = (test['dt'] - test['fault_time_1']).dt.days\n",
    "test.loc[test.days_1 <= 0,'tag'] = None\n",
    "test.loc[test.days_1 <= 0,'days_1'] = None\n",
    "test['days_2'] = (test['fault_time_1'] - test['dt_first']).dt.days\n",
    "test.loc[test.fault_time_1 >= test.dt,'days_2'] = None\n",
    "\n",
    "\n",
    "test['serial_number'] = test['serial_number'].apply(lambda x:int(x.split('_')[1]))\n",
    "test_x = test[feature_name]\n",
    "clf = LGBMClassifier(\n",
    "    learning_rate=0.001,\n",
    "    n_estimators=1500,\n",
    "    num_leaves=125,\n",
    "    max_depth=7,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=2019,\n",
    "    bagging_fraction= 0.9,\n",
    "    bagging_freq= 8,\n",
    "    lambda_l1 = 0.5,\n",
    "    lambda_l2 = 0,\n",
    "    cat_smooth = 10, \n",
    "    is_unbalenced = 'True',\n",
    "    metric=None\n",
    "#     learning_rate=0.001,\n",
    "#     n_estimators=10000,\n",
    "#     num_leaves=127,\n",
    "#     max_depth=8,\n",
    "#     subsample=0.8,\n",
    "#     colsample_bytree=0.8,\n",
    "#     random_state=2019,\n",
    "#     is_unbalenced = 'True',\n",
    "#     metric=None\n",
    ")\n",
    "print('************** training **************')\n",
    "print(train_df.shape,test_x.shape)\n",
    "clf.fit(\n",
    "    train_df, labels,\n",
    "    eval_set=[(train_df, labels)],\n",
    "    eval_metric='auc',\n",
    "    early_stopping_rounds=10,\n",
    "    verbose=100\n",
    ")\n",
    "test['p'] = clf.predict_proba(test_x)[:,1]\n",
    "test['label'] = test['p'].rank()\n",
    "test['label']= (test['label']>=test.shape[0] * 0.990).astype(int)\n",
    "submit = test.loc[test.label == 1]\n",
    "submit = submit.sort_values('p',ascending=False)\n",
    "submit = submit.drop_duplicates(['serial_number','model'])\n",
    "submit['serial_number'] = submit['serial_number'].astype(str)\n",
    "submit['serial_number'] = ['disk_'+ x for x in submit['serial_number']]\n",
    "submit = submit[0:90]\n",
    "submit[['manufacturer','model','serial_number','dt']].to_csv(\"../prediction_result/predictions.csv\",index=False,header = None)\n",
    "submit.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
