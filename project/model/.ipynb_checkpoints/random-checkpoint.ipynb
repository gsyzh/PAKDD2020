{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 512/512 [00:01<00:00, 341.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************** training **************\n",
      "(1053632, 51) (1376226, 51)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's auc: 0.841684\n",
      "[200]\tvalid_0's auc: 0.868435\n",
      "[300]\tvalid_0's auc: 0.887182\n",
      "[400]\tvalid_0's auc: 0.903385\n",
      "[500]\tvalid_0's auc: 0.916215\n",
      "[600]\tvalid_0's auc: 0.92596\n",
      "[700]\tvalid_0's auc: 0.93677\n",
      "[800]\tvalid_0's auc: 0.944151\n",
      "[900]\tvalid_0's auc: 0.947726\n",
      "[1000]\tvalid_0's auc: 0.950654\n",
      "[1100]\tvalid_0's auc: 0.953154\n",
      "[1200]\tvalid_0's auc: 0.954386\n",
      "Early stopping, best iteration is:\n",
      "[1225]\tvalid_0's auc: 0.954487\n"
     ]
    }
   ],
   "source": [
    "#随机取样进行合并的做法\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "\n",
    "def lower_sample_data(df, percent=100):\n",
    "    data1 = df[df['label'] == 0]  # 将多数类别的样本放在data1\n",
    "    data0 = df[df['label'] == 1]  # 将少数类别的样本放在data0\n",
    "    index = np.random.randint(\n",
    "        len(data1), size=percent * (len(df) - len(data1)))  # 随机给定下采样取出样本的序号\n",
    "    lower_data1 = data1.iloc[list(index)]  # 下采样\n",
    "    return(pd.concat([lower_data1, data0]))\n",
    "\n",
    "train_2018_6 = joblib.load('D:/python/ml/seven/disk_sample_smart_log_2018_Q2/2018_6.jl.z')\n",
    "\n",
    "train_2018_6=lower_sample_data(train_2018_6)\n",
    "train_2018_6.to_csv(\"train_2018_6.csv\",index=False)\n",
    "del train_2018_6\n",
    "\n",
    "train_2018_5 = joblib.load('D:/python/ml/seven/disk_sample_smart_log_2018_Q2/2018_5.jl.z')\n",
    "train_2018_5=lower_sample_data(train_2018_5)\n",
    "train_2018_5.to_csv(\"train_2018_5.csv\",index=False)\n",
    "del train_2018_5\n",
    "\n",
    "train_2018_4 = joblib.load('D:/python/ml/seven/disk_sample_smart_log_2018_Q2/2018_4.jl.z')\n",
    "train_2018_4=lower_sample_data(train_2018_4)\n",
    "\n",
    "train_2018_4.to_csv(\"train_2018_4.csv\",index=False)\n",
    "del train_2018_4\n",
    "\n",
    "train_2018_3 = joblib.load('D:/python/ml/seven/disk_sample_smart_log_2018_Q1/2018_3.jl.z')\n",
    "train_2018_3=lower_sample_data(train_2018_3)\n",
    "\n",
    "train_2018_3.to_csv(\"train_2018_3.csv\",index=False)\n",
    "del train_2018_3\n",
    "\n",
    "train_2018_2 = joblib.load('D:/python/ml/seven/disk_sample_smart_log_2018_Q1/2018_2.jl.z')\n",
    "train_2018_2=lower_sample_data(train_2018_2)\n",
    "\n",
    "train_2018_2.to_csv(\"train_2018_2.csv\",index=False)\n",
    "del train_2018_2\n",
    "\n",
    "train_2018_1 = joblib.load('D:/python/ml/seven/disk_sample_smart_log_2018_Q1/2018_1.jl.z')\n",
    "train_2018_1=lower_sample_data(train_2018_1)\n",
    "\n",
    "train_2018_1.to_csv(\"train_2018_1.csv\",index=False)\n",
    "del train_2018_1\n",
    "\n",
    "train_2017_7 = joblib.load('D:/python/ml/seven/disk_sample_smart_log_2017/2017_7.jl.z')\n",
    "train_2017_7=lower_sample_data(train_2017_7)\n",
    "train_2017_7.to_csv(\"train_2017_7.csv\",index=False)\n",
    "del train_2017_7\n",
    "\n",
    "train_2017_8 = joblib.load('D:/python/ml/seven/disk_sample_smart_log_2017/2017_8.jl.z')\n",
    "train_2017_8=lower_sample_data(train_2017_8)\n",
    "train_2017_8.to_csv(\"train_2017_8.csv\",index=False)\n",
    "del train_2017_8\n",
    "\n",
    "train_2017_9 = joblib.load('D:/python/ml/seven/disk_sample_smart_log_2017/2017_9.jl.z')\n",
    "train_2017_9=lower_sample_data(train_2017_9)\n",
    "train_2017_9.to_csv(\"train_2017_9.csv\",index=False)\n",
    "del train_2017_9\n",
    "\n",
    "train_2017_10 = joblib.load('D:/python/ml/seven/disk_sample_smart_log_2017/2017_10.jl.z')\n",
    "train_2017_10=lower_sample_data(train_2017_10)\n",
    "train_2017_10.to_csv(\"train_2017_10.csv\",index=False)\n",
    "del train_2017_10\n",
    "\n",
    "train_2017_11 = joblib.load('D:/python/ml/seven/disk_sample_smart_log_2017/2017_11.jl.z')\n",
    "train_2017_11=lower_sample_data(train_2017_11)\n",
    "train_2017_11.to_csv(\"train_2017_11.csv\",index=False)\n",
    "del train_2017_11\n",
    "\n",
    "train_2017_12 = joblib.load('D:/python/ml/seven/disk_sample_smart_log_2017/2017_12.jl.z')\n",
    "train_2017_12=lower_sample_data(train_2017_12)\n",
    "train_2017_12.to_csv(\"train_2017_12.csv\",index=False)\n",
    "del train_2017_12\n",
    "\n",
    "#生成新的文件\n",
    "train_2018_4 = pd.read_csv('train_2018_4.csv')\n",
    "train_2018_5 = pd.read_csv('train_2018_5.csv')\n",
    "train_2018_6 = pd.read_csv('train_2018_6.csv')\n",
    "train_x = train_2018_4\n",
    "train_x = train_x.append(train_2018_5).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2018_6).reset_index(drop=True)\n",
    "train_x.to_csv(\"train_x.csv\",index=False)\n",
    "\n",
    "train_2018_2 = pd.read_csv('train_2018_2.csv')\n",
    "train_2018_3 = pd.read_csv('train_2018_3.csv')\n",
    "val_x = train_2018_2\n",
    "val_x = train_x.append(train_2018_3).reset_index(drop=True)\n",
    "val_x.to_csv(\"val_x.csv\",index=False)\n",
    "\n",
    "train_2017_7 = pd.read_csv('train_2017_7.csv')\n",
    "train_2017_8 = pd.read_csv('train_2017_8.csv')\n",
    "train_2017_9 = pd.read_csv('train_2017_9.csv')\n",
    "train_2017_10 = pd.read_csv('train_2017_10.csv')\n",
    "train_2017_11 = pd.read_csv('train_2017_11.csv')\n",
    "train_2017_12 = pd.read_csv('train_2017_12.csv')\n",
    "train_2018_1 = pd.read_csv('train_2018_1.csv')\n",
    "train_2018_2 = pd.read_csv('train_2018_2.csv')\n",
    "train_2018_3 = pd.read_csv('train_2018_3.csv')\n",
    "train_2018_4 = pd.read_csv('train_2018_4.csv')\n",
    "train_2018_5 = pd.read_csv('train_2018_5.csv')\n",
    "train_2018_6 = pd.read_csv('train_2018_6.csv')\n",
    "train_x = train_2017_7\n",
    "train_x = train_x.append(train_2017_8).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2017_9).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2017_10).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2017_11).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2017_12).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2018_1).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2018_2).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2018_3).reset_index(drop=True)\n",
    "train_x = train_x.append(train_2018_4).reset_index(drop=True)\n",
    "train_x.to_csv(\"train_df.csv\",index=False)\n",
    "\n",
    "#训练数据\n",
    "test = pd.read_csv('D:/python/ml/seven/disk_sample_smart_log_test_a.csv/test_a.csv')\n",
    "tag = pd.read_csv('D:/python/ml/seven/disk_sample_fault_tag.csv/tag.csv')\n",
    "test['dt'] = test['dt'].apply(lambda x:''.join(str(x)[0:4] +'-'+ str(x)[4:6]  +'-'+ str(x)[6:]))\n",
    "test['dt'] = pd.to_datetime(test['dt'])\n",
    "tag['fault_time'] = pd.to_datetime(tag['fault_time'])\n",
    "\n",
    "test = test.sort_values(['serial_number','dt'])\n",
    "test = test.drop_duplicates().reset_index(drop=True)\n",
    "sub = test[['manufacturer','model','serial_number','dt']]\n",
    "\n",
    "drop_list =[]\n",
    "for i in tqdm([col for col in test.columns if col not in ['manufacturer','model']]):\n",
    "    if (test[i].nunique() == 1)&(test[i].isnull().sum() == 0):\n",
    "        drop_list.append(i)\n",
    "\n",
    "df= pd.DataFrame()\n",
    "df['fea'] = test.isnull().sum().index\n",
    "df['isnull_sum'] = test.isnull().sum().values\n",
    "fea_list = list(set(df.loc[df.isnull_sum != test.shape[0]]['fea']) - set(drop_list))\n",
    "test = test[fea_list]\n",
    "\n",
    "serial = pd.read_csv('serial.csv')\n",
    "serial.columns = ['serial_number','dt_first','model']\n",
    "serial.dt_first = pd.to_datetime(serial.dt_first)\n",
    "\n",
    "tag['tag'] = tag['tag'].astype(str)\n",
    "tag = tag.groupby(['serial_number','fault_time','model'])['tag'].apply(lambda x :'|'.join(x)).reset_index()\n",
    "tag.columns = ['serial_number','fault_time_1','model','tag']\n",
    "map_dict = dict(zip(tag['tag'].unique(), range(tag['tag'].nunique())))\n",
    "tag['tag'] = tag['tag'].map(map_dict).fillna(-1).astype('int32')\n",
    "\n",
    "###用到的特征\n",
    "feature_name = [i for i in test.columns if i not in ['dt','manufacturer']] + ['days','days_1','days_2','tag']\n",
    "\n",
    "train_x = pd.read_csv('train_x.csv')\n",
    "train_x = train_x.merge(serial,how = 'left',on = ['serial_number','model'])\n",
    "###硬盘的使用时常\n",
    "train_x['dt'] = pd.to_datetime(train_x['dt'],format = '%Y-%m-%d %H:%M:%S')\n",
    "train_x['days'] = (train_x['dt'] - train_x['dt_first']).dt.days\n",
    "\n",
    "train_x = train_x.merge(tag,how = 'left',on = ['serial_number','model'])\n",
    "###当前时间与另一个model故障的时间差，\n",
    "train_x['days_1'] = (train_x['dt'] - train_x['fault_time_1']).dt.days\n",
    "train_x.loc[train_x.days_1 <= 0,'tag'] = None\n",
    "train_x.loc[train_x.days_1 <= 0,'days_1'] = None\n",
    "\n",
    "###同一硬盘第一次使用到开始故障的时间\n",
    "train_x['days_2'] = (train_x['fault_time_1'] - train_x['dt_first']).dt.days\n",
    "train_x.loc[train_x.fault_time_1 >= train_x.dt,'days_2'] = None\n",
    "\n",
    "train_x['serial_number'] = train_x['serial_number'].apply(lambda x:int(x.split('_')[1]))\n",
    "train_y = train_x.label.values\n",
    "train_x = train_x[feature_name]\n",
    "gc.collect()\n",
    "\n",
    "val_x = pd.read_csv('val_x.csv')\n",
    "val_x = val_x.merge(serial,how = 'left',on = ['serial_number','model'])\n",
    "val_x['dt'] = pd.to_datetime(val_x['dt'],format = '%Y-%m-%d %H:%M:%S')\n",
    "val_x['dt_first'] = pd.to_datetime(val_x['dt_first'],format = '%Y-%m-%d %H:%M:%S')\n",
    "val_x['days'] = (val_x['dt'] - val_x['dt_first']).dt.days\n",
    "\n",
    "\n",
    "val_x = val_x.merge(tag,how = 'left',on = ['serial_number','model'])\n",
    "val_x['days_1'] = (val_x['dt'] - val_x['fault_time_1']).dt.days\n",
    "val_x.loc[val_x.days_1 <= 0,'tag'] = None\n",
    "val_x.loc[val_x.days_1 <= 0,'days_1'] = None\n",
    "val_x['days_2'] = (val_x['fault_time_1'] - val_x['dt_first']).dt.days\n",
    "val_x.loc[val_x.fault_time_1 >= val_x.dt,'days_2'] = None\n",
    "\n",
    "val_x['serial_number'] = val_x['serial_number'].apply(lambda x:int(x.split('_')[1]))\n",
    "val_y = val_x.label.values\n",
    "val_x = val_x[feature_name]\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "clf = LGBMClassifier(\n",
    "    learning_rate=0.001,\n",
    "    n_estimators=10000,\n",
    "    num_leaves=127,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=2019,\n",
    "    is_unbalenced = 'True',\n",
    "    metric=None\n",
    ")\n",
    "print('************** training **************')\n",
    "print(train_x.shape,val_x.shape)\n",
    "clf.fit(\n",
    "    train_x, train_y,\n",
    "    eval_set=[(val_x, val_y)],\n",
    "    eval_metric='auc',\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=100\n",
    ")\n",
    "train_df = pd.read_csv('train_df.csv')\n",
    "train_df = train_df.merge(serial,how = 'left',on = ['serial_number','model'])\n",
    "train_df['dt'] = pd.to_datetime(train_df['dt'],format = '%Y-%m-%d %H:%M:%S')\n",
    "train_df['dt_first'] = pd.to_datetime(train_df['dt_first'],format = '%Y-%m-%d %H:%M:%S')\n",
    "train_df['days'] = (train_df['dt'] - train_df['dt_first']).dt.days\n",
    "\n",
    "train_df = train_df.merge(tag,how = 'left',on = ['serial_number','model'])\n",
    "train_df['days_1'] = (train_df['dt'] - train_df['fault_time_1']).dt.days\n",
    "train_df.loc[train_df.days_1 <= 0,'tag'] = None\n",
    "train_df.loc[train_df.days_1 <= 0,'days_1'] = None\n",
    "train_df['days_2'] = (train_df['fault_time_1'] - train_df['dt_first']).dt.days\n",
    "train_df.loc[train_df.fault_time_1 >= train_df.dt,'days_2'] = None\n",
    "\n",
    "\n",
    "train_df['serial_number'] = train_df['serial_number'].apply(lambda x:int(x.split('_')[1]))\n",
    "labels = train_df.label.values\n",
    "train_df = train_df[feature_name]\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "test = test.merge(serial,how = 'left',on =  ['serial_number','model'])\n",
    "test['days'] = (test['dt'] - test['dt_first']).dt.days\n",
    "\n",
    "test = test.merge(tag,how = 'left',on = ['serial_number','model'])\n",
    "test['days_1'] = (test['dt'] - test['fault_time_1']).dt.days\n",
    "test.loc[test.days_1 <= 0,'tag'] = None\n",
    "test.loc[test.days_1 <= 0,'days_1'] = None\n",
    "test['days_2'] = (test['fault_time_1'] - test['dt_first']).dt.days\n",
    "test.loc[test.fault_time_1 >= test.dt,'days_2'] = None\n",
    "\n",
    "\n",
    "test['serial_number'] = test['serial_number'].apply(lambda x:int(x.split('_')[1]))\n",
    "test_x = test[feature_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************** training **************\n",
      "(2284115, 51) (178028, 51)\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's auc: 0.783764\n",
      "[200]\ttraining's auc: 0.818645\n",
      "[300]\ttraining's auc: 0.845872\n",
      "[400]\ttraining's auc: 0.860809\n",
      "[500]\ttraining's auc: 0.879912\n",
      "[600]\ttraining's auc: 0.891059\n",
      "[700]\ttraining's auc: 0.910467\n",
      "[800]\ttraining's auc: 0.921917\n",
      "[900]\ttraining's auc: 0.930645\n",
      "[1000]\ttraining's auc: 0.939783\n",
      "[1100]\ttraining's auc: 0.945555\n",
      "[1200]\ttraining's auc: 0.951584\n",
      "[1300]\ttraining's auc: 0.961696\n",
      "[1400]\ttraining's auc: 0.970046\n",
      "[1500]\ttraining's auc: 0.97544\n",
      "[1600]\ttraining's auc: 0.979928\n",
      "[1700]\ttraining's auc: 0.982922\n",
      "[1800]\ttraining's auc: 0.985721\n",
      "[1900]\ttraining's auc: 0.987164\n",
      "[2000]\ttraining's auc: 0.988676\n",
      "[2100]\ttraining's auc: 0.989894\n",
      "[2200]\ttraining's auc: 0.990614\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "clf = LGBMClassifier(\n",
    "    learning_rate=0.001,\n",
    "    n_estimators=5000,\n",
    "    num_leaves=127,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=2019,\n",
    "#     is_unbalenced = 'True',\n",
    "    metric=None\n",
    ")\n",
    "print('************** training **************')\n",
    "print(train_df.shape,test_x.shape)\n",
    "clf.fit(\n",
    "    train_df, labels,\n",
    "    eval_set=[(train_df, labels)],\n",
    "    eval_metric='auc',\n",
    "    early_stopping_rounds=10,\n",
    "    verbose=100\n",
    ")\n",
    "# joblib.dump(clf, 'clf.pkl')\n",
    "print('xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')\n",
    "\n",
    "test['p'] = clf.predict_proba(test_x)[:,1]\n",
    "test['label'] = test['p'].rank()\n",
    "test['label']= (test['label']>=test.shape[0] * 0.990).astype(int)\n",
    "submit = test.loc[test.label == 1]\n",
    "submit = submit.sort_values('p',ascending=False)\n",
    "submit = submit.drop_duplicates(['serial_number','model'])\n",
    "submit['serial_number'] = submit['serial_number'].astype(str)\n",
    "submit['serial_number'] = ['disk_'+ x for x in submit['serial_number']]\n",
    "submit[['manufacturer','model','serial_number','dt','p']].to_csv(\"result3142.csv\",index=False,header = None)\n",
    "submit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clf0314.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf, 'clf0314.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('result312d2.csv')\n",
    "submit = submit.drop_duplicates('serial_number')\n",
    "submit.to_csv(\"result312d1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
